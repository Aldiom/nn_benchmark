{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CP-decomposition with tensor power method\n",
    "This notebook runs through an example process of CP decomposition. In this case, we will use ResNet50. The first thing to do is to load the model and the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#mod_name = 'mobilenet'\n",
    "mod_name = 'resnet50'\n",
    "#mod_name = 'vgg16'\n",
    "model = tf.keras.models.load_model('../models/ResNet50.h5')\n",
    "model = model.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagenet\n",
    "\n",
    "train_ds = imagenet.load_ds((224,224))\n",
    "if mod_name == 'mobilenet':\n",
    "    train_ds = train_ds.map(lambda x,y: (tf.keras.applications.mobilenet_v2.preprocess_input(x), y),\n",
    "                            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "elif mod_name == 'resnet50':\n",
    "    train_ds = train_ds.map(lambda x,y: (tf.nn.bias_add(x, [-123.68, -116.779, -103.939]) ,y),\n",
    "                            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "elif mod_name == 'vgg16':\n",
    "    train_ds = train_ds.map(lambda x,y: (tf.keras.applications.imagenet_utils.preprocess_input(x,\n",
    "                                        mode='torch'), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_ds = train_ds.cache()\n",
    "train_ds = train_ds.shuffle(5000).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decompostion method has 2 phases: rank estimation and the actual decomposition. To estimate rank we decompose the whole network assigning a rank of 5 to all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting initial rank dict for loss measurements\n",
    "ranks = dict()\n",
    "laynames = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "        if isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
    "            pass\n",
    "        else: \n",
    "            ranks[layer.name] = 5\n",
    "            laynames.append(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dsize = 50000      \n",
    "orig_acc = 0.9245 #0.9245 resnet 0.9046 mobnet 0.9106 vgg\n",
    "#model.compile(loss='sparse_categorical_crossentropy',\n",
    "#              metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy()])\n",
    "#_, orig_acc = model.evaluate(train_ds.batch(64), steps=dsize//64+1)\n",
    "\n",
    "c_losses = dict()\n",
    "d_losses = dict()\n",
    "old_model = model\n",
    "\n",
    "for i in range(len(ranks)):\n",
    "    curr_ranks = dict()\n",
    "    for j in laynames[0:i+1]:\n",
    "        curr_ranks[j] = ranks[j]\n",
    "    dec_model = CP_optimize(old_model, curr_ranks)\n",
    "    dec_model.compile(optimizer=tf.keras.optimizers.SGD(1e-4, momentum=0.9),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy()])\n",
    "    bsz = 64\n",
    "    ds = train_ds.batch(bsz)\n",
    "    # we measure the accuracy of the model at this stage\n",
    "    dec_model.fit(ds, steps_per_epoch=dsize//bsz+1) \n",
    "    _, acc = dec_model.evaluate(ds, steps=dsize//bsz+1)  \n",
    "    if isinstance(old_model.get_layer(laynames[i]), tf.keras.layers.Conv2D):\n",
    "        c_losses[laynames[i]] = orig_acc - acc\n",
    "    else:\n",
    "        d_losses[laynames[i]] = orig_acc - acc\n",
    "    del old_model\n",
    "    old_model = dec_model\n",
    "    del dec_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook the accuracy of the original network and the accuracy losses for each layer are already measured to save time, but the code used is included in case you want to try again with something different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precalculated losses for imagenet val set, use them if you want to save time\n",
    "\n",
    "if mod_name == 'mobilenet':\n",
    "    c_losses = {'block_5_expand': 0.4155, 'block_2_project': 0.8937, 'block_12_expand': 0.8193,\n",
    "                'block_1_expand': 0.4621, 'block_12_project': 0.8844, 'block_4_expand': 0.8683,\n",
    "                'block_13_project': 0.8988, 'block_11_expand': 0.8746, 'block_6_project': 0.8973,\n",
    "                'block_3_project': 0.8972, 'block_9_expand': 0.3242, 'block_7_project': 0.7914,\n",
    "                'block_10_expand': 0.5996, 'block_14_expand': 0.8979, 'block_10_project': 0.8935,\n",
    "                'block_7_expand': 0.8828, 'block_14_project': 0.8958, 'block_1_project': 0.8962,\n",
    "                'Conv_1': 0.8895, 'block_4_project': 0.7647, 'Conv1': 0.0924, 'block_13_expand': 0.8577,\n",
    "                'block_3_expand': 0.7847, 'expanded_conv_project': 0.8981, 'block_9_project': 0.3996,\n",
    "                'block_8_project': 0.4170, 'block_16_expand': 0.8765, 'block_2_expand': 0.8829, \n",
    "                'block_5_project': 0.3563, 'block_6_expand': 0.5946, 'block_15_project': 0.8926, \n",
    "                'block_8_expand': 0.5607, 'block_16_project': 0.8960, 'block_15_expand': 0.8879, \n",
    "                'block_11_project': 0.8817}\n",
    "    d_losses = {'predictions': 0.8806}\n",
    "elif mod_name == 'resnet50':\n",
    "    c_losses = {'conv4_block1_0_conv': 0.2273, 'conv2_block1_1_conv': -0.0045, 'conv2_block2_2_conv': -0.0182,\n",
    "             'conv2_block3_2_conv': -0.0207, 'conv2_block1_3_conv': -0.0085, 'conv4_block1_2_conv': 0.0539, \n",
    "             'conv5_block2_2_conv': 0.6786, 'conv3_block3_1_conv': -0.0103, 'conv4_block4_2_conv': 0.2060, \n",
    "             'conv4_block3_3_conv': 0.1362, 'conv5_block3_1_conv': 0.7771, 'conv2_block2_3_conv': -0.0261, \n",
    "             'conv3_block4_1_conv': 0.0084, 'conv4_block3_2_conv': 0.1776, 'conv2_block3_1_conv': -0.0318, \n",
    "             'conv2_block2_1_conv': -0.0207, 'conv1_conv': 0.0042, 'conv3_block3_3_conv': -0.0166, \n",
    "             'conv3_block2_2_conv': 0.0179, 'conv5_block1_0_conv': 0.6709, 'conv3_block2_1_conv': 0.0093, \n",
    "             'conv4_block1_1_conv': 0.0247, 'conv4_block5_2_conv': 0.2173, 'conv4_block4_3_conv': 0.1755, \n",
    "             'conv4_block2_3_conv': 0.1486, 'conv4_block6_3_conv': 0.2816, 'conv3_block4_2_conv': 0.0310, \n",
    "             'conv5_block3_2_conv': 0.7877, 'conv3_block3_2_conv': -0.0064, 'conv4_block2_2_conv': 0.1924, \n",
    "             'conv4_block6_1_conv': 0.2759, 'conv3_block4_3_conv': 0.0094, 'conv5_block3_3_conv': 0.7747, \n",
    "             'conv2_block3_3_conv': -0.0290, 'conv3_block1_1_conv': -0.0330, 'conv4_block4_1_conv': 0.1903, \n",
    "             'conv4_block1_3_conv': 0.1939, 'conv3_block1_3_conv': 0.0232, 'conv4_block5_1_conv': 0.2371, \n",
    "             'conv5_block2_1_conv': 0.6987, 'conv4_block5_3_conv': 0.1825, 'conv4_block6_2_conv': 0.3194, \n",
    "             'conv3_block1_0_conv': 0.0493, 'conv5_block1_3_conv': 0.6562, 'conv5_block2_3_conv': 0.6573, \n",
    "             'conv5_block1_1_conv': 0.4090, 'conv2_block1_2_conv': -0.0043, 'conv3_block2_3_conv': 0.0015, \n",
    "             'conv4_block2_1_conv': 0.1752, 'conv2_block1_0_conv': -0.0003, 'conv3_block1_2_conv': -0.0163, \n",
    "             'conv5_block1_2_conv': 0.4568, 'conv4_block3_1_conv': 0.1720}\n",
    "    d_losses = {'predictions': 0.8903}\n",
    "elif mod_name == 'vgg16':\n",
    "    c_losses = {'block5_conv3': 0.8361, 'block1_conv1': -0.0190, 'block1_conv2': 0.0200, \n",
    "                'block5_conv2': 0.8152, 'block5_conv1': 0.7409, 'block3_conv1': 0.1064, \n",
    "                'block4_conv2': 0.6069, 'block2_conv2': 0.0401, 'block3_conv2': 0.1969, \n",
    "                'block4_conv3': 0.6990, 'block4_conv1': 0.4949, 'block3_conv3': 0.2242, \n",
    "                'block2_conv1': 0.0262}\n",
    "    d_losses = {'predictions': 0.8725, 'fc2': 0.8683, 'fc1': 0.8749}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the calculated losses, we assign a rank to each layer, proportional to the loss. Convolutional layers and FC (Dense) layers have their ranks assigned separately. For Conv layers, we give an average rank of 150 for the network, whereas the average rank for FC layers is 300, just like in the original paper. Keep in mind that later in the process we will detect and NOT decompose the layers that would be actually slowed down by the decomposition, so the average rank could be higher than the specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rank calculation\n",
    "c_total_ranks = 150 * len(c_losses)\n",
    "c_total_loss = sum(c_losses.values())\n",
    "d_total_ranks = 300 * len(d_losses)\n",
    "d_total_loss = sum(d_losses.values())\n",
    "\n",
    "for i in range(len(laynames)):\n",
    "    name = laynames[i] \n",
    "    if name in c_losses:\n",
    "        curr_rank = max(int(150 * len(c_losses) * c_losses[name] / c_total_loss), 5)\n",
    "        ranks[name] = curr_rank\n",
    "        c_total_ranks -= curr_rank\n",
    "    else:\n",
    "        curr_rank = max(int(300 * len(d_losses) * d_losses[name] / d_total_loss), 5)\n",
    "        ranks[name] = curr_rank\n",
    "        d_total_ranks -= curr_rank       \n",
    "while c_total_ranks > 0:\n",
    "    maxscore = 0 # Sainte-Lague method to proportionately assign ranks\n",
    "    for i in c_losses.keys():\n",
    "        score = c_losses[i] / (2 * ranks[i] + 1)\n",
    "        if maxscore < score:\n",
    "            maxscore = score\n",
    "            max_i = i\n",
    "    ranks[max_i] += 1\n",
    "    c_total_ranks -= 1\n",
    "while d_total_ranks > 0:\n",
    "    maxscore = 0\n",
    "    for i in d_losses.keys():\n",
    "        score = d_losses[i] / (2 * ranks[i] + 1)\n",
    "        if maxscore < score:\n",
    "            maxscore = score\n",
    "            max_i = i\n",
    "    ranks[max_i] += 1\n",
    "    d_total_ranks -= 1\n",
    "\n",
    "# eliminate from the process layers that would be slowed down by it\n",
    "for i in laynames: # Keras dxdxCxN -> dxdxR CxR RxN --- NCD2WoHo > R(CWiHi+D2WoHo+NWoHo) speedup\n",
    "    layer = model.get_layer(i)\n",
    "    R = ranks[i]\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        _, Wi, Hi, C = layer.input_shape\n",
    "        _, Wo, Ho, N = layer.output_shape\n",
    "        D = layer.kernel_size[0]\n",
    "        if R * (C*Wi*Hi + D**2*Wo*Ho + N*Wo*Ho) >= N*C*D**2*Wo*Ho:\n",
    "            ranks.pop(i)\n",
    "    else:\n",
    "        _, C = layer.input_shape\n",
    "        _, N = layer.output_shape\n",
    "        if R >= C*N/(C+N): # K < CN/(C+N) to accelerate\n",
    "            ranks.pop(i)\n",
    "\n",
    "print('Ranks:\\n', ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we decompose again the network, this time with the definitive ranks, optimizing each layer one-by-one and training for a single epoch for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network decomposition\n",
    "\n",
    "old_model = model\n",
    "del model\n",
    "for i in range(len(ranks)):\n",
    "    print('Stage %d/%d' % (i+1, len(ranks)), 16*'-')\n",
    "    curr_ranks = dict()\n",
    "    for j in laynames:\n",
    "        if j in ranks:\n",
    "            curr_ranks[j] = ranks[j]\n",
    "            if len(curr_ranks) > i:\n",
    "                break\n",
    "\n",
    "    dec_model = CP_optimize(old_model, curr_ranks)\n",
    "    del old_model\n",
    "    dec_model.compile(optimizer=tf.keras.optimizers.SGD(1e-4, momentum=0.9),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy()])\n",
    "    bsz = 64\n",
    "    ds = train_ds.batch(bsz)\n",
    "    dec_model.fit(ds, steps_per_epoch=dsize//bsz+1)\n",
    "    if i+1 < len(ranks):\n",
    "        old_model = dec_model\n",
    "        del dec_model\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally add the preprocessing layers to the model and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_model.summary()\n",
    "\n",
    "netin = keras.Input((224,224,3), dtype='uint8')\n",
    "x = tf.cast(netin, 'float32')\n",
    "if mod_name == 'mobilenet':\n",
    "    x = keras.applications.mobilenet_v2.preprocess_input(x) \n",
    "elif mod_name == 'resnet50':\n",
    "    x = tf.nn.bias_add(x, [-123.68, -116.779, -103.939])\n",
    "elif mod_name == 'vgg16'\n",
    "    x = keras.applications.imagenet_utils.preprocess_input(x, mode='torch')\n",
    "x = dec_model(x)\n",
    "dec_model = keras.Model(inputs=netin, outputs=x)\n",
    "dec_model.save('%s_CP' % mod_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TF)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
